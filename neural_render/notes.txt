2021-05-15
Started many jobs as we now have a decent prototype (of the most basic thing).
Seems like wider is better. No multiplier is way worse. 
mse vs perceptual loss probably doesn't matter much, will keep perceptual for now.
Celu for output doesn't seem better, just keep Relu.
More mul size doesn't seem better.
Larger output multiplier doesn't seem better.

2021-05-16
Residual connection is important (no real surprise here...)
Actually, more mul size is better (and maybe 1:1 is the right ratio).
Deeper is slightly better (but maybe not best way to make net bigger).
Results around different output block and wider are questionable because
of different batch size (use same batch size).
Doesn't seem like hidden size expansion accomplishes anything (this is
pretty surprising to me).

For bizarre and unclear reasons:
  - large hidden feed forward with multiply: fine
  - large hidden feed forward with multiply + output extension: fine
  - not large (1x multiplier) hidden feed forward with multiply + output
    extension: doesn't train
  - not large (1x multiplier) hidden feed forward with multiply: bad (but does
    train a bit)

So conclusion is just use 1 multiply per block. Also probably worth looking
into output extension somewhat...

Layer norm isn't very important, but it does seem slightly better and
it reduces noise a bit (maybe?)

